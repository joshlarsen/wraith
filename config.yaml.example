# Example configuration for wraith vulnerability classifier

firestore:
  project_id: "your-gcp-project-id"
  database: "(default)"  # Optional: specify Firestore database name, defaults to "(default)"
  collection: "vulnerability_classifications"

llm:
  model: "gpt-4o-mini"  # OpenAI model to use
  api_key: "your-openai-api-key-here"
  # base_url: "https://api.openai.com/v1"  # Optional: custom base URL for OpenAI-compatible APIs

osv:
  modified_csv_url: "https://osv-vulnerabilities.storage.googleapis.com/modified_id.csv"
  api_url: "https://api.osv.dev/v1"
  ecosystem: "npm"  # Optional: filter by ecosystem (npm, PyPI, Go, etc.)
  cache_dir: ".cache/osv"  # Optional: directory for CSV cache files, defaults to ".cache/osv"
  cache_ttl: 24  # Optional: cache TTL in hours, defaults to 24 hours, 0 = no expiration

# Examples of custom base URLs for OpenAI-compatible services:
#
# For Azure OpenAI:
# llm:
#   model: "gpt-4"
#   api_key: "your-azure-api-key"
#   base_url: "https://your-resource.openai.azure.com/v1"
#
# For local LLM server (like Ollama with OpenAI compatibility):
# llm:
#   model: "llama3"
#   api_key: "not-needed-for-local"
#   base_url: "http://localhost:11434/v1"